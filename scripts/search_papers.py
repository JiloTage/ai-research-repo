#!/usr/bin/env python3
"""ArXivè«–æ–‡æ¤œç´¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆ - HSEARLé–¢é€£è«–æ–‡ã®è‡ªå‹•æ¤œç´¢."""

import feedparser
import re
import urllib.parse
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Set


def get_hsearl_search_keywords() -> Dict[str, List[str]]:
    """HSEARL 6ã¤ã®è¦³ç‚¹ã«åŸºã¥ãæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—.
    
    Returns:
        Dict[str, List[str]]: å„è¦³ç‚¹ã¨ãã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
    """
    return {
        "èªçŸ¥å‹": [
            "cognitive models", "personality types", "MBTI", "cognitive functions",
            "information processing", "thinking styles", "decision making",
            "cognitive assessment", "psychological types"
        ],
        "å‹•æ©Ÿå‹": [
            "personality motivation", "enneagram", "motivation theory",
            "behavioral drives", "personality dynamics", "core motivations",
            "fear and desire", "personality assessment"
        ],
        "åå¿œå‹": [
            "emotional response", "tritype", "emotional intelligence",
            "stress response", "coping mechanisms", "emotional regulation",
            "behavioral patterns", "response styles"
        ],
        "é©å¿œçŠ¶æ…‹": [
            "psychological health", "adaptation", "mental health levels",
            "stress tolerance", "resilience", "psychological well-being",
            "adaptive behavior", "coping strategies"
        ],
        "è‚²ã¡": [
            "developmental psychology", "childhood development", "family dynamics",
            "early experiences", "attachment theory", "parenting styles",
            "developmental trauma", "family systems"
        ],
        "ãƒ­ãƒ¼ãƒ«": [
            "social roles", "role theory", "social identity", "role adaptation",
            "social masks", "persona", "social performance", "role flexibility"
        ]
    }


def search_arxiv_papers(keywords: List[str], days_back: int = 7) -> List[Dict]:
    """ArXivã§è«–æ–‡ã‚’æ¤œç´¢.
    
    Args:
        keywords: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
        days_back: ä½•æ—¥å‰ã‹ã‚‰ã®è«–æ–‡ã‚’æ¤œç´¢ã™ã‚‹ã‹
        
    Returns:
        List[Dict]: è¦‹ã¤ã‹ã£ãŸè«–æ–‡ã®æƒ…å ±ãƒªã‚¹ãƒˆ
    """
    papers = []
    
    # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ (YYYY-MM-DDå½¢å¼)
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days_back)
    
    for keyword in keywords:
        # ArXiv APIã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        query = f'all:"{keyword}" AND (cat:cs.AI OR cat:cs.CL OR cat:cs.LG OR cat:stat.ML)'
        encoded_query = urllib.parse.quote(query)
        
        # ArXiv API URL
        url = f"http://export.arxiv.org/api/query?search_query={encoded_query}&start=0&max_results=10&sortBy=submittedDate&sortOrder=descending"
        
        try:
            # ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—
            feed = feedparser.parse(url)
            
            for entry in feed.entries:
                # è«–æ–‡æƒ…å ±ã‚’æŠ½å‡º
                paper_info = {
                    "id": entry.id.split("/")[-1],
                    "title": entry.title.replace("\n", " ").strip(),
                    "authors": [author.name for author in getattr(entry, "authors", [])],
                    "summary": entry.summary.replace("\n", " ").strip(),
                    "published": entry.published,
                    "arxiv_url": entry.id,
                    "pdf_url": entry.id.replace("abs", "pdf"),
                    "search_keyword": keyword,
                    "categories": [tag.term for tag in getattr(entry, "tags", [])]
                }
                papers.append(paper_info)
                
        except Exception as e:
            print(f"è­¦å‘Š: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{keyword}' ã®æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    return papers


def filter_relevant_papers(papers: List[Dict], existing_ids: Set[str]) -> List[Dict]:
    """é–¢é€£æ€§ã®é«˜ã„è«–æ–‡ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°.
    
    Args:
        papers: æ¤œç´¢ã•ã‚ŒãŸè«–æ–‡ã®ãƒªã‚¹ãƒˆ
        existing_ids: æ—¢ã«å‡¦ç†æ¸ˆã¿ã®è«–æ–‡IDã®ã‚»ãƒƒãƒˆ
        
    Returns:
        List[Dict]: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸè«–æ–‡ã®ãƒªã‚¹ãƒˆ
    """
    # é‡è¤‡é™¤å»
    seen_ids = set()
    unique_papers = []
    
    for paper in papers:
        paper_id = paper["id"]
        if paper_id not in seen_ids and paper_id not in existing_ids:
            seen_ids.add(paper_id)
            unique_papers.append(paper)
    
    # HSEARLé–¢é€£æ€§ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    hsearl_keywords = [
        "personality", "cognitive", "emotional", "motivation", "development",
        "adaptation", "social", "psychological", "behavior", "individual differences",
        "mental health", "assessment", "modeling", "human factors"
    ]
    
    relevant_papers = []
    for paper in unique_papers:
        # ã‚¿ã‚¤ãƒˆãƒ«ã¨è¦ç´„ã‚’å°æ–‡å­—ã§çµåˆ
        text = (paper["title"] + " " + paper["summary"]).lower()
        
        # é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if any(keyword in text for keyword in hsearl_keywords):
            relevant_papers.append(paper)
    
    return relevant_papers


def load_existing_paper_ids(todo_file: Path) -> Set[str]:
    """æ—¢å­˜ã®ã‚µãƒ¼ãƒ™ã‚¤äºˆå®šè«–æ–‡ãƒªã‚¹ãƒˆã‹ã‚‰IDã‚’èª­ã¿è¾¼ã¿.
    
    Args:
        todo_file: ã‚µãƒ¼ãƒ™ã‚¤äºˆå®šè«–æ–‡ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        Set[str]: æ—¢å­˜ã®è«–æ–‡IDã®ã‚»ãƒƒãƒˆ
    """
    existing_ids = set()
    
    if todo_file.exists():
        content = todo_file.read_text(encoding="utf-8")
        # ArXiv IDãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢ (ä¾‹: 2301.12345)
        pattern = r"arxiv\.org/abs/(\d{4}\.\d{4,5})"
        matches = re.findall(pattern, content)
        existing_ids.update(matches)
    
    return existing_ids


def update_survey_todo_list(papers: List[Dict], todo_file: Path) -> None:
    """ã‚µãƒ¼ãƒ™ã‚¤äºˆå®šè«–æ–‡ãƒªã‚¹ãƒˆã‚’æ›´æ–°.
    
    Args:
        papers: æ–°ã—ã„è«–æ–‡ã®ãƒªã‚¹ãƒˆ
        todo_file: ã‚µãƒ¼ãƒ™ã‚¤äºˆå®šè«–æ–‡ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    if not papers:
        print("æ–°ã—ã„è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    # æ—¢å­˜ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’èª­ã¿è¾¼ã¿
    if todo_file.exists():
        content = todo_file.read_text(encoding="utf-8")
    else:
        content = """# ã‚µãƒ¼ãƒ™ã‚¤äºˆå®šè«–æ–‡ãƒªã‚¹ãƒˆ

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€HSEARLç ”ç©¶ã«é–¢é€£ã™ã‚‹è«–æ–‡ã®ã‚µãƒ¼ãƒ™ã‚¤å¾…ã¡ãƒªã‚¹ãƒˆã§ã™ã€‚
Claude Codeã«ã‚ˆã£ã¦è«–æ–‡ã‚µãƒãƒªãƒ¼ãŒç”Ÿæˆã•ã‚Œã‚‹ã¨ã€è©²å½“é …ç›®ãŒãƒã‚§ãƒƒã‚¯ã•ã‚Œã¾ã™ã€‚

## å‡¦ç†å¾…ã¡è«–æ–‡

"""
    
    # æ–°ã—ã„è«–æ–‡ã‚’è¿½åŠ 
    new_entries = []
    for paper in papers:
        entry = f"""
### {paper['title']}
- **ArXiv ID**: {paper['id']}
- **URL**: {paper['arxiv_url']}
- **è‘—è€…**: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}
- **å…¬é–‹æ—¥**: {paper['published'][:10]}
- **æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: {paper['search_keyword']}
- **ã‚«ãƒ†ã‚´ãƒª**: {', '.join(paper['categories'][:3])}
- **è¦ç´„**: {paper['summary'][:200]}...

**ã‚µãƒãƒªãƒ¼ç”Ÿæˆ**: [ ] æœªå‡¦ç†

---
"""
        new_entries.append(entry)
    
    # "å‡¦ç†å¾…ã¡è«–æ–‡"ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å¾Œã«æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªã‚’æŒ¿å…¥
    if "## å‡¦ç†å¾…ã¡è«–æ–‡" in content:
        parts = content.split("## å‡¦ç†å¾…ã¡è«–æ–‡")
        if len(parts) >= 2:
            # æ—¢å­˜ã®å‡¦ç†å¾…ã¡è«–æ–‡ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å¾Œã«æ–°ã—ã„è«–æ–‡ã‚’è¿½åŠ 
            updated_content = parts[0] + "## å‡¦ç†å¾…ã¡è«–æ–‡\n" + "".join(new_entries) + parts[1]
        else:
            updated_content = content + "".join(new_entries)
    else:
        updated_content = content + "\n## å‡¦ç†å¾…ã¡è«–æ–‡\n" + "".join(new_entries)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
    todo_file.write_text(updated_content, encoding="utf-8")
    print(f"âœ… {len(papers)}ä»¶ã®æ–°ã—ã„è«–æ–‡ã‚’ã‚µãƒ¼ãƒ™ã‚¤äºˆå®šãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¾ã—ãŸã€‚")


def main() -> None:
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†."""
    print("ğŸ” HSEARLé–¢é€£è«–æ–‡æ¤œç´¢ã‚’é–‹å§‹...")
    
    # ãƒ‘ã‚¹ã®è¨­å®š
    project_root = Path(__file__).parent.parent
    todo_file = project_root / "docs" / "ã‚µãƒ¼ãƒ™ã‚¤äºˆå®šè«–æ–‡ãƒªã‚¹ãƒˆ.md"
    
    # æ—¢å­˜ã®è«–æ–‡IDã‚’èª­ã¿è¾¼ã¿
    existing_ids = load_existing_paper_ids(todo_file)
    print(f"ğŸ“‹ æ—¢å­˜ã®è«–æ–‡æ•°: {len(existing_ids)}ä»¶")
    
    # HSEARLè¦³ç‚¹ã§ã®æ¤œç´¢
    keywords_by_aspect = get_hsearl_search_keywords()
    all_papers = []
    
    for aspect, keywords in keywords_by_aspect.items():
        print(f"ğŸ” {aspect}é–¢é€£è«–æ–‡ã‚’æ¤œç´¢ä¸­...")
        papers = search_arxiv_papers(keywords, days_back=14)  # 2é€±é–“åˆ†
        print(f"   {len(papers)}ä»¶ã®è«–æ–‡ã‚’ç™ºè¦‹")
        all_papers.extend(papers)
    
    # é–¢é€£æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    print(f"ğŸ“Š åˆè¨ˆ {len(all_papers)}ä»¶ã®è«–æ–‡ã‚’ç™ºè¦‹")
    relevant_papers = filter_relevant_papers(all_papers, existing_ids)
    print(f"âœ¨ ãã®ã†ã¡ {len(relevant_papers)}ä»¶ãŒæ–°è¦ã§é–¢é€£æ€§ãŒé«˜ã„è«–æ–‡ã§ã™")
    
    # ã‚µãƒ¼ãƒ™ã‚¤äºˆå®šãƒªã‚¹ãƒˆã‚’æ›´æ–°
    if relevant_papers:
        update_survey_todo_list(relevant_papers, todo_file)
        
        # Claude Codeã‚¿ã‚¹ã‚¯ç”Ÿæˆã®ãŸã‚ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        print("\nğŸ¤– Claude Codeã•ã‚“ã€ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print("1. docs/prompts/paper_summary_prompt.mdã®æ‰‹é †ã«å¾“ã£ã¦è«–æ–‡ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ")
        print("2. HSEARLã¨ã®é–¢é€£æ€§ã‚’åˆ†æ")
        print("3. docs/surveys/ã«ä¿å­˜")
        print(f"4. å¯¾è±¡è«–æ–‡æ•°: {len(relevant_papers)}ä»¶")
    else:
        print("æ–°ã—ã„é–¢é€£è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")


if __name__ == "__main__":
    main()